defaults:
  - ../default

task_name: "WalkerWalk"
robot_name: "walker"

algo: "ppo"

observation_space:
  shape: [6]
observation_shape: [6]

ppo:
  action_num: 6
  multi_gpu: ${experiment.multi_gpu}
  num_actors: 32
  e_clip: 0.2
  clip_value: 0.2
  entropy_coef: 0.01
  critic_coef: 0.5
  bounds_loss_coef: 1.0
  gamma: 0.99
  tau: 0.95
  truncate_grads: True
  grad_norm: 0.5
  value_bootstrap: True
  normalize_advantage: True
  normalize_input: True
  normalize_value: True
  reward_scale_value: 1.0
  clip_value_loss: True
  horizon_length: 4
  minibatch_size: 128
  mini_epochs: 2
  learning_rate: 0.0003
  lr_schedule: "linear"
  max_agent_steps: 100000000
  kl_threshold: 0.008
  save_frequency: 10
  save_best_after: 50

  network:
    mlp:
      units: [32, 32]
    separate_value_mlp: True
