defaults:
  - ../default

task_name: "WalkerWalk"
robot_name: "walker"

algo: "dreamer"

observation_space:
  shape: [64, 64, 3]
observation_shape: [64, 64, 3]

dreamer:
  # Experience collection
  num_actors: 32  # Number of parallel actors
  action_num: 6
  horizon_length: 64  # Number of steps to collect per iteration

  # Training batch parameters
  batch_size: 32  # Number of sequences per batch
  batch_length: 64  # Sequence length for training
  train_ratio: 1.0  # Ratio of training steps to environment steps

  # World model parameters
  dyn_stoch: 32  # Size of stochastic state
  dyn_deter: 512  # Size of deterministic state
  dyn_hidden: 512  # Size of hidden layers in dynamics network
  dyn_rec_depth: 1  # Number of recurrent layers
  dyn_discrete: 32  # Size of discrete latent (0 for continuous)

  # Activation functions
  act: "elu"  # Activation function
  norm: "none"  # Normalization type
  dyn_mean_act: "none"  # Activation for mean in RSSM
  dyn_std_act: "softplus"  # Activation for std in RSSM
  dyn_min_std: 0.1  # Minimum standard deviation

  # KL balancing
  kl_free: 1.0  # Free bits for KL loss
  kl_scale: 1.0  # Scale for KL loss
  dyn_scale: 0.5  # Scale for dynamics loss
  rep_scale: 0.1  # Scale for representation loss

  # Policy parameters
  gamma: 0.997  # Discount factor
  lambda: 0.95  # GAE parameter
  horizon: 15  # Planning horizon for actor training
  actor_dist: "normal"  # Actor distribution
  actor_entropy: 0.001  # Entropy regularization
  actor_init_std: 1.0  # Initial standard deviation
  actor_min_std: 0.1  # Minimum standard deviation
  actor_max_std: 1.0  # Maximum standard deviation
  actor_grad: "reinforce"  # Gradient type for actor (dynamics or reinforce)

  # Optimization
  model_lr: 3e-4  # World model learning rate
  actor_lr: 8e-5  # Actor learning rate
  critic_lr: 8e-5  # Critic learning rate
  weight_decay: 0.0  # Weight decay
  grad_clip: 100.0  # Gradient clipping

  # Network architecture
  units: 512  # Size of hidden layers
  reward_layers: 4  # Number of layers in reward head
  cont_layers: 4  # Number of layers in continue head
  actor_layers: 4  # Number of layers in actor
  critic_layers: 4  # Number of layers in critic

  # Architecture settings
  encoder:
    hidden_dims: [512, 512, 512]  # Hidden layer dimensions
    activation: "elu"  # Activation function
    norm: "none"  # Normalization type

  decoder:
    hidden_dims: [512, 512, 512]  # Hidden layer dimensions
    activation: "elu"  # Activation function
    norm: "none"  # Normalization type

  # Component loss scales
  reward_scale: 1.0  # Scale for reward prediction loss
  cont_scale: 1.0  # Scale for continue prediction loss
  decoder_scale: 1.0  # Scale for decoder loss

  # Training schedule and monitoring
  log_every: 1000  # Log frequency
  eval_episodes: 10  # Number of episodes for evaluation
  max_agent_steps: 2000000  # Maximum number of agent steps
  save_frequency: 100  # Checkpoint save frequency
